{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a778139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # Main PyTorch library\n",
    "from torch import nn  # Neural network components for building models\n",
    "\n",
    "import torchvision  # Library with tools for computer vision\n",
    "from torchvision.transforms import ToTensor, transforms  # Tools to process images\n",
    "from torch.utils.data import DataLoader, Subset, Dataset, random_split  # Tools to handle data\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from torchsummary import summary  # Prints a nice summary of your model\n",
    "from tqdm import tqdm  # Adds progress bars to your loops\n",
    "\n",
    "import matplotlib as plt  # Library for creating charts and visuals\n",
    "import matplotlib.pyplot as plt  # The part of matplotlib for actually drawing plots\n",
    "import numpy as np  # For working with arrays and math operations\n",
    "import pandas as pd\n",
    "\n",
    "import nibabel as nib\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "print(torch.__version__)  # Shows which version of PyTorch you're using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4043f53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "file_path = '/Users/baumn/Downloads/commercial_mri_nacc69.csv'  # Replace with the actual file path\n",
    "#columns_to_read = ['AMYLPET', 'AMYLCSF', 'TAUPETAD', 'CSFTAU', 'NACCHTNC', 'NACCHTNC', 'NACCACEI', 'NACCAAAS', 'NACCBETA', 'NACCCCBS', 'NACCDIUR', 'NACCVASD', 'NACCANGI', 'NACCLIPL', 'NACCNSD', 'NACCAC', 'NACCADEP', 'NACCAPSY', 'NACCAANX', 'NACCADMD', 'NACCPDMD', 'NACCEMD', 'NACCEPMD', 'NACCDBMD']\n",
    "columns_to_read = [ \"NACCID\", \"GRAYVOL\",\n",
    "    \"LCAC\", \"RCAC\", \"LCMF\", \"RCMF\", \"LCUN\", \"RCUN\", \"LENT\", \"RENT\",\n",
    "    \"LFUS\", \"RFUS\", \"LINFPAR\", \"RINFPAR\", \"LINFTEMP\", \"RINFTEMP\",\n",
    "    \"LINSULA\", \"RINSULA\", \"LISTHC\", \"RISTHC\", \"LLATOCC\", \"RLATOCC\",\n",
    "    \"LLATORBF\", \"RLATORBF\", \"LLING\", \"RLING\", \"LMEDORBF\", \"RMEDORBF\",\n",
    "    \"LMIDTEMP\", \"RMIDTEMP\", \"LPARCEN\", \"RPARCEN\", \"LPARHIP\", \"RPARHIP\",\n",
    "    \"LPARSOP\", \"RPARSOP\", \"LPARORB\", \"RPARORB\", \"LPARTRI\", \"RPARTRI\",\n",
    "    \"LPERCAL\", \"RPERCAL\", \"LPOSCEN\", \"RPOSCEN\", \"LPOSCIN\", \"RPOSCIN\",\n",
    "    \"LPRECEN\", \"RPRECEN\", \"LPRECUN\", \"RPRECUN\", \"LROSANC\", \"RROSANC\",\n",
    "    \"LROSMF\", \"RROSMF\", \"LSUPFR\", \"RSUPFR\", \"LSUPPAR\", \"RSUPPAR\",\n",
    "    \"LSUPTEM\", \"RSUPTEM\", \"LSUPMAR\", \"RSUPMAR\", \"LTRTEM\", \"RTRTEM\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(file_path, usecols=columns_to_read)\n",
    "\n",
    "df = df[df[\"LCAC\"] != 88.8888]\n",
    "\n",
    "print(df.head())\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1383e6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def list_immediate_folders(path):\n",
    "    folders = []\n",
    "    for item in os.listdir(path):\n",
    "        item_path = os.path.join(path, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            folders.append(item)\n",
    "    return folders\n",
    "\n",
    "# Example usage\n",
    "parent_folder = \"/Users/baumn/Downloads/nifti2\" # Replace with the actual path\n",
    "subfolders = list_immediate_folders(parent_folder)\n",
    "print(subfolders)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eba0021",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_x = []\n",
    "for i in subfolders:\n",
    "    for x in df[\"NACCID\"]:\n",
    "        if x in i:\n",
    "            list_x.append(x)\n",
    "print(set(list_x))\n",
    "print(len(set(list_x)))\n",
    "\n",
    "list2 = []\n",
    "for i in subfolders:\n",
    "    for x in df[\"NACCID\"]:\n",
    "        if x in i:\n",
    "            list2.append(i)\n",
    "print(set(list2))\n",
    "print(len(set(list2)))\n",
    "\n",
    "x=0\n",
    "for i in set(list_x):\n",
    "    if i == \"NACC234794\":\n",
    "        x+=1\n",
    "\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cda91a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[df[\"NACCID\"].isin(set(list_x))]    \n",
    "df2\n",
    "df2['VALUE'] = np.where(df2['GRAYVOL'] > 600, 1, 0)\n",
    "\n",
    "print(df2.head())\n",
    "df2.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9167597e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.conv_layers = nn.Sequential(\n",
    "        #we have 2 layers. the first one detects low level info like edges, textures, and basic patterns.\n",
    "        #second detects higher ones like shapes structre ofject parts.\n",
    "\n",
    "        nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1),  # Conv layer (3 input channels -> 16 filters)\n",
    "        nn.BatchNorm2d(16),  # This stabilizes training by reducing internal covariate shift,\n",
    "        # Ignore BatchNorm for now, focus on understanding (Conv, Relu, Pooling)\n",
    "        nn.ReLU(),  # Activation function for non-linearity\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),  # Downsampling (reduces feature map size)\n",
    "\n",
    "        nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1),  # Conv layer (16 filters -> 32 filters)\n",
    "        nn.BatchNorm2d(32),  # Batch normalization for stable training\n",
    "        nn.ReLU(),  # Activation function\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),  # Downsampling\n",
    "\n",
    "        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),  # Conv layer (32 filters -> 64 filters)\n",
    "        nn.BatchNorm2d(64),  # Batch normalization for stable training\n",
    "        nn.ReLU(),  # Activation function\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),  # Downsampling\n",
    "        #instead of learning all features in one big convolutional later, we gradually increase number of filters\n",
    "\n",
    "        nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),  # Conv layer (64 filters -> 128 filters)\n",
    "        nn.BatchNorm2d(128),  # Batch normalization for stable training\n",
    "        nn.ReLU(),  # Activation function\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),  # Downsampling\n",
    "        #instead of learning all features in one big convolutional later, we gradually increase number of filters\n",
    "\n",
    "        nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),  # Conv layer (128 filters -> 256 filters)\n",
    "        nn.BatchNorm2d(256),  # Batch normalization for stable training\n",
    "        nn.ReLU(),  # Activation function\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2)  # Downsampling\n",
    "        #instead of learning all features in one big convolutional later, we gradually increase number of filters\n",
    "    )\n",
    " \n",
    "\n",
    "    self.flatten = nn.Flatten()  # Flattens feature maps into a 1D vector\n",
    "\n",
    "    self.fully_connected = nn.Sequential(\n",
    "        nn.Linear(in_features=256*8*8, out_features=128),  # Fully connected layer (flattened input -> 128 neurons)\n",
    "        nn.ReLU(),  # Activation function\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(in_features=128, out_features=2),  # Output layer (2 classes for classification)\n",
    "    ) #Make it simpler (binary)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv_layers(x)  # Pass input through convolutional layers\n",
    "    #print(f'Shape coming out of convnet will be: {x.shape}')  # Debugging output\n",
    "    x = self.flatten(x)  # Flatten feature maps for fully connected layers\n",
    "    x = self.fully_connected(x)  # Pass through fully connected layers\n",
    "    return x\n",
    "\n",
    "model = ConvNet()  # Create an instance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2368023",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_read2 = [ \n",
    "    \"VALUE\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8da8625",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyData(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for loading neuroimaging data (.nii or .nii.gz) with labels from a preloaded DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        root_dir (str): Directory containing NIfTI files.\n",
    "        labels_df (DataFrame): Pandas DataFrame with at least 'subject_id' and 'label' columns.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, labels_df, label_column):\n",
    "        self.root_dir = root_dir\n",
    "        self.labels_df = labels_df\n",
    "        self.label_column = label_column\n",
    "        self.samples = self.make_dataset()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        nifti_data = nib.load(img_path)\n",
    "        data = nifti_data.get_fdata()\n",
    "\n",
    "        if data.ndim == 4:\n",
    "            data = data[:, :, :, 0]\n",
    "\n",
    "        if data.ndim != 3:\n",
    "            raise ValueError(f\"Expected 3D volume, got shape {data.shape} at {img_path}\")\n",
    "\n",
    "        mid_slice = data.shape[2] // 2\n",
    "        slice_2d = data[:, :, mid_slice]\n",
    "        if slice_2d.ndim != 2:\n",
    "            raise ValueError(f\"Expected 2D slice, got shape {slice_2d.shape} at {img_path}\")\n",
    "\n",
    "\n",
    "        slice_2d = (slice_2d - slice_2d.min()) / (slice_2d.max() - slice_2d.min() + 1e-5)\n",
    "\n",
    "        #slice_2d = np.array(slice_2d, dtype=np.float32)\n",
    "\n",
    "\n",
    "\n",
    "        image_tensor = torch.tensor(slice_2d, dtype=torch.float32)\n",
    "        image_tensor=image_tensor.unsqueeze(0).unsqueeze(0)  # shape: [1, D, H, W]\n",
    "\n",
    "        if image_tensor.ndim != 4:\n",
    "            raise ValueError(f\"Before interpolation, shape is {image_tensor.shape}\")\n",
    "    \n",
    "        image_tensor = F.interpolate(image_tensor, size=(256,256), mode='bilinear', align_corners=False)\n",
    "        image_tensor = image_tensor.squeeze(0)\n",
    "\n",
    "        label_tensor = torch.tensor(label, dtype=torch.long).squeeze()\n",
    "        \n",
    "        return image_tensor, label_tensor\n",
    "\n",
    "    def make_dataset(self):\n",
    "        samples = []\n",
    "        for dirpath, _, filenames in os.walk(self.root_dir):\n",
    "            for file in filenames:\n",
    "                if file.endswith(\".nii.gz\") or file.endswith(\".nii\"):\n",
    "                    path = os.path.join(dirpath, file)\n",
    "                    subject_id = os.path.basename(os.path.dirname(path))[:10]\n",
    "\n",
    "                    label_row = self.labels_df[self.labels_df['NACCID'] == subject_id]\n",
    "                    if not label_row.empty:\n",
    "                        label = label_row.iloc[0][self.label_column]\n",
    "                        # Ensure label is a 1D array\n",
    "                        if isinstance(label, (np.ndarray, list, tuple)):\n",
    "                            label = np.array(label, dtype=np.float32)\n",
    "                        else:\n",
    "                            label = np.array([label], dtype=np.float32)\n",
    "                        samples.append((path, label))\n",
    "                    else:\n",
    "                        print(f\"Warning: No label found for subject ID {subject_id}\")\n",
    "        return samples\n",
    "\n",
    "\n",
    "    def extract_id_from_filename(self, fname):\n",
    "        \"\"\"\n",
    "        Extract the subject ID from the filename.\n",
    "        Modify this depending on your filename structure.\n",
    "        Example: 'sub-12345678_something.nii.gz' -> 'sub-12345678'\n",
    "        \"\"\"\n",
    "        return fname.split('_')[0]  # adjust based on your actual filename pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1f8f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyData(root_dir='/Users/baumn/Downloads/nifti3', labels_df=df2, label_column = columns_to_read2)\n",
    "print(f\"Number of samples: {len(dataset)}\")\n",
    "#print(dataset.data.shape)\n",
    "#print(dataset[1][1])\n",
    "#image, label = dataset[0]\n",
    "#print(image)\n",
    "#print(label)\n",
    "#print(\"Image shape:\", image.shape)        # e.g. torch.Size([1, D, H, W])\n",
    "#print(\"Label:\", label)\n",
    "#print(\"Label shape:\", label.shape)\n",
    "#print(\"Image dtype:\", image.dtype)\n",
    "#print(\"Label dtype:\", label.dtype)\n",
    "\n",
    "\n",
    "#for i in range(len(dataset)):\n",
    "    #image, label = dataset[i]\n",
    "    #print(f\"Sample {i}: label={label.tolist()}, shape={label.shape}\")\n",
    "\n",
    "#print(dataset.label)\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "#print(dataloader.shape)\n",
    "\n",
    "for x, y in train_loader:\n",
    "    print(f\"Batch image shape: {x.shape}\")  # [batch_size, 1, D, H, W]\n",
    "    print(f\"Batch labels: {y}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4897d393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ConvNet().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # For classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7f62d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(train_dataloader, model, loss_fn, optimizer, epochs):\n",
    "  model.train() # Put the model in training mode\n",
    "  train_loss = []\n",
    "  for epoch in range(epochs):\n",
    "    train_loss_epoch = 0\n",
    "    for image, label in tqdm(train_dataloader, desc=\"Training Model\"):\n",
    "      image, label = image.to(device), label.to(device).long()\n",
    "      # print(f'Shape of the images: {image.shape}')\n",
    "      # print(f'Shape of the labels: {label.shape}')\n",
    "      # print(label)\n",
    "      # input()\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      pred = model(image)\n",
    "      #print(pred)\n",
    "      #print(pred.shape)\n",
    "      #print(label.shape)\n",
    "      #input()\n",
    "      loss = loss_fn(pred, label) # Prediction has to go first (Calculate loss between the prediction and the true value)\n",
    "      loss.backward()\n",
    "      train_loss_epoch += loss.item()\n",
    "\n",
    "      optimizer.step()\n",
    "\n",
    "    avg_loss = train_loss_epoch / len(train_dataloader)\n",
    "    train_loss.append(avg_loss)\n",
    "\n",
    "    print(f'Epoch: {epoch+1} | Loss: {avg_loss:.4f}')\n",
    "\n",
    "  return train_loss\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9253c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 25\n",
    "losses = train_loop(train_loader, model, criterion, optimizer, epochs=num_epochs) # Lets take a few minute break while it trains and we will come back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6933584e",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_list = list(range(1, num_epochs+1))\n",
    "plt.plot(epoch_list, losses)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ccd37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(correct, total):\n",
    "  return correct/total * 100\n",
    "\n",
    "def test_loop(test_dataloader, model):\n",
    "  model.eval() # Put the model in evaluation mode\n",
    "  correct = 0\n",
    "  total = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for image, label in tqdm(test_dataloader, desc=\"Testing Model\"):\n",
    "      image, label = image.to(device), label.to(device)\n",
    "\n",
    "      pred = model(image)\n",
    "\n",
    "      correct += (pred.argmax(1) == label).type(torch.float).sum().item()\n",
    "      total += len(label)\n",
    "\n",
    "    print(f'Accuracy: {accuracy(correct, total)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e5ce71",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loop(test_loader, model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c84296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "\n",
    "        self.activations = None\n",
    "        self.gradients = None\n",
    "\n",
    "        # Register hook to get gradients during backprop\n",
    "        self.hook_handles = []\n",
    "        self._register_hooks()\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        def forward_hook(module, input, output):\n",
    "            self.activations = output.detach()\n",
    "\n",
    "        def backward_hook(module, grad_in, grad_out):\n",
    "            self.gradients = grad_out[0].detach()\n",
    "\n",
    "        self.hook_handles.append(self.target_layer.register_forward_hook(forward_hook))\n",
    "        self.hook_handles.append(self.target_layer.register_full_backward_hook(backward_hook))\n",
    "\n",
    "    def remove_hooks(self):\n",
    "        for handle in self.hook_handles:\n",
    "            handle.remove()\n",
    "\n",
    "    def generate(self, input_tensor, class_idx=None):\n",
    "        self.model.eval()\n",
    "        output = self.model(input_tensor)\n",
    "\n",
    "        if class_idx is None:\n",
    "            class_idx = torch.argmax(output, dim=1).item()\n",
    "\n",
    "        self.model.zero_grad()\n",
    "        class_score = output[0, class_idx]\n",
    "        class_score.backward()\n",
    "\n",
    "        # Grad-CAM calculation\n",
    "        pooled_grads = torch.mean(self.gradients, dim=[0, 2, 3])\n",
    "        for i in range(self.activations.shape[1]):\n",
    "            self.activations[0, i, :, :] *= pooled_grads[i]\n",
    "\n",
    "        heatmap = torch.mean(self.activations, dim=1).squeeze()\n",
    "        heatmap = np.maximum(heatmap.cpu().numpy(), 0)\n",
    "        heatmap /= (heatmap.max() + 1e-8)\n",
    "        return heatmap\n",
    "\n",
    "def show_gradcam_on_image(input_tensor, heatmap):\n",
    "    image_np = input_tensor.squeeze().cpu().numpy()\n",
    "    heatmap_resized = cv2.resize(heatmap, (image_np.shape[1], image_np.shape[0]))\n",
    "    heatmap_colored = cv2.applyColorMap(np.uint8(255 * heatmap_resized), cv2.COLORMAP_JET)\n",
    "\n",
    "    # Convert grayscale to 3-channel for overlay\n",
    "    image_3ch = np.stack([image_np * 255] * 3, axis=-1).astype(np.uint8)\n",
    "\n",
    "    overlay = cv2.addWeighted(heatmap_colored, 0.4, image_3ch, 0.6, 0)\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.imshow(image_np, cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title(\"Grad-CAM Heatmap\")\n",
    "    plt.imshow(heatmap_resized, cmap='jet')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title(\"Overlay\")\n",
    "    plt.imshow(overlay)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Get a sample image and label from your dataset\n",
    "image, label = dataset[0]\n",
    "input_tensor = image.unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "# Identify the last conv layer from model\n",
    "\n",
    "target_layer = model.conv_layers[16]  # Last Conv2d\n",
    "\n",
    "# Apply Grad-CAM\n",
    "gradcam = GradCAM(model, target_layer)\n",
    "heatmap = gradcam.generate(input_tensor)\n",
    "gradcam.remove_hooks()\n",
    "\n",
    "# Visualize\n",
    "show_gradcam_on_image(image, heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef3c7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# After testing/predicting\n",
    "y_true = []\n",
    "y_pred = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images.to(device))\n",
    "        preds = torch.argmax(outputs, dim=1).cpu()\n",
    "        y_pred.extend(preds.numpy())\n",
    "        y_true.extend(labels.numpy())\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef06f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "\n",
    "def plot_precision_recall(model, dataloader, device):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_prob = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)  # raw logits\n",
    "            probs = torch.softmax(outputs, dim=1)  # convert to probabilities\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_prob.extend(probs[:, 1].cpu().numpy())  # take prob for class 1\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_prob = np.array(y_prob)\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
    "    ap = average_precision_score(y_true, y_prob)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recall, precision, label=f'AP = {ap:.2f}', color='blue')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precisionâ€“Recall Curve')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_precision_recall(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66c9fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def plot_roc_curve(model, dataloader, device):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_prob = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)  # raw logits\n",
    "            probs = torch.softmax(outputs, dim=1)  # convert to probabilities\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_prob.extend(probs[:, 1].cpu().numpy())  # probability of class 1\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_prob = np.array(y_prob)\n",
    "\n",
    "    # Compute ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'AUC = {roc_auc:.2f}')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=1)  # random baseline\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_roc_curve(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9842e73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "metrics = {\n",
    "    \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "    \"Precision\": precision_score(y_true, y_pred),\n",
    "    \"Recall\": recall_score(y_true, y_pred),\n",
    "    \"F1 Score\": f1_score(y_true, y_pred),\n",
    "}\n",
    "\n",
    "print(accuracy_score(y_true, y_pred))\n",
    "\n",
    "plt.bar(metrics.keys(), metrics.values(), color='skyblue')\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"Evaluation Metrics\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.grid(axis='y')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
